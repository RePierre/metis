{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Analysis of corpus files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Import directives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Common components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We need a generator class to iterate over the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, corpus_dir):\n",
    "        self._corpus_dir = corpus_dir\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_name in self._get_file_names():\n",
    "            try:\n",
    "                yield self._get_file_text(file_name)\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                continue\n",
    "            except TypeError:\n",
    "                continue\n",
    "\n",
    "    def _get_file_names(self):\n",
    "        for root, dirs, files in os.walk(self._corpus_dir):\n",
    "            for f in files:\n",
    "                yield os.path.join(root, f)\n",
    "\n",
    "    def _get_file_text(self, file_name):\n",
    "        text = []\n",
    "        with open(file_name, 'rt') as f:\n",
    "            data = json.load(f)\n",
    "            for section in data['body']:\n",
    "                text.append(section['title'])\n",
    "                text.append(section['text'])\n",
    "            return '\\n'.join(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Plotting utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We need a method to transform topic distribution matrix into data points which can be plotted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_points(topic_distribution, cutoff=0.4):\n",
    "    x, y, z = [], [], []\n",
    "    for (doc_ix, topic_ix), prob in np.ndenumerate(topic_distribution):\n",
    "        if prob > cutoff:\n",
    "            x.append(topic_ix)\n",
    "            z.append(doc_ix)\n",
    "            y.append(prob)\n",
    "    return np.asarray(x), np.asarray(y), np.asarray(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "And a method to actually plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_data_points(x, y, z=0):\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    if not isinstance(z, np.ndarray):\n",
    "        plt.scatter(x, y)\n",
    "    else:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(x, y, z)\n",
    "        ax.set_xlabel('Topic')\n",
    "        ax.set_ylabel('Probability')\n",
    "        ax.set_zlabel('Document')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Finally we need to know the path to the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "corpus_dir = input('Corpus directory:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Truncated Singular Value Decomposition on Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(corpus_dir)\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "bow = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "t_svd = TruncatedSVD()\n",
    "t_svd.fit(bow)\n",
    "bow = t_svd.transform(bow)\n",
    "x, y = ([_x for _x, _y in bow], [_y for _x, _y in bow])\n",
    "\n",
    "plot_data_points(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Truncated Singular Value Decomposition on TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "tfidf_tsvd = t_svd.transform(tfidf)\n",
    "x, y = ([_x for _x, _y in tfidf_tsvd], [_y for _x, _y in tfidf_tsvd])\n",
    "plot_data_points(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Document to topic distribution using Latent Dirichlet Allocation on TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Running `Latent Dirichlet Allocation` with 150 clusters as per the results of [running Hierarchical Dirichlet Process analysis](../analysis/hdp.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_lda = LatentDirichletAllocation(n_components=150, learning_method='batch')\n",
    "tfidf_distrib = tfidf_lda.fit_transform(tfidf)\n",
    "\n",
    "x, y, z = get_data_points(tfidf_distrib)\n",
    "\n",
    "plot_data_points(x, y, z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Document to topic distribution using Latent Dirichlet Allocation on Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The negative values from `Bag of Words` need to be clipped in order to have a proper `Latent Dirichlet Allocation` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "bow = bow.clip(min=0)\n",
    "\n",
    "bow_lda = LatentDirichletAllocation(n_components=150, learning_method='batch')\n",
    "bow_distrib = bow_lda.fit_transform(bow)\n",
    "\n",
    "x, y, z = get_data_points(bow_distrib)\n",
    "\n",
    "plot_data_points(x, y, z)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "name": "analysis-of-corpus-files.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
